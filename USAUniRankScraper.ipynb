{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53530f73-d8fb-4436-93a4-d257e87c1485",
   "metadata": {},
   "source": [
    "# USA University/Colleges Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f314e26-b0f9-41ba-ae20-c2bd71809e0e",
   "metadata": {},
   "source": [
    "**Scrapes the Uni123 Rank website (https://www.4icu.org/us/)**\n",
    "\n",
    "USNews would be preferred but has extensive web-scrape blocking measures \n",
    "\n",
    "* Data Collection: Requests is used for simple webscraping as this website did not have extensive blocking features at this time\n",
    "* Data Extraction: Beautiful soup is used to parse HTML elements saved as lists\n",
    "* Data Storage: Scraped elements are stored in a dataframe for further processing.\n",
    "* Data Cleaning: Only small cleaning step required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357364a0-8a68-4f77-8276-e41a01e4171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5725be-d2ce-49d7-8f95-018a4d8c2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping https://www.4icu.org/us/ Uni123 Rank website because USNews was too difficult\n",
    "url = \"https://www.4icu.org/us/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6b2f9-4301-4035-aaa5-8010e59d32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_divs = soup.find_all('tr')  # Find all divs with the given class\n",
    "print(f\"Number of divs found: {len(school_divs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464da50-e2d4-419e-ab36-fb989b08424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_divs_test = school_divs[2]\n",
    "school_divs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebe0fe-0195-4b47-a5e5-2c1f57a4388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = []\n",
    "school = []\n",
    "city = []\n",
    "\n",
    "for div in school_divs:\n",
    "    tds = div.find_all('td')\n",
    "    \n",
    "    if len(tds) >= 3:\n",
    "        rank.append(tds[0].text)\n",
    "        school.append(tds[1].text)\n",
    "        city.append(tds[2].text)\n",
    "    \n",
    "# Create df from dictionary\n",
    "df = pd.DataFrame({'Rank': rank, 'School': school, 'City': city})\n",
    "df['City'] = df['City'].replace(' ...', '', regex=True)\n",
    "\n",
    "df['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n",
    "maxrank = df['Rank'].max()\n",
    "df['Rank'] = df['Rank'].fillna(maxrank + 1)\n",
    "df['Rank'] = df['Rank'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03493c3-fad8-4beb-989c-e99016b69d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a dataframe\n",
    "df.to_csv('USA_Uni_Rank.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca4277-f405-4438-8555-fda859f13823",
   "metadata": {},
   "source": [
    "# Scraping to obtain State Names along with University/College Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fb9c6-8505-4769-84d1-cb11f5b88ffb",
   "metadata": {},
   "source": [
    "**Scraping from StudyAbroad.Shiksha.com to obtain States names along with University names \n",
    "\n",
    "(https://studyabroad.shiksha.com/usa/universities-10)\n",
    "\n",
    "* Data Collection: Selenium must be used here again\n",
    "* Data Storage: Scraped elements are stored in a dataframe for further processing.\n",
    "* Data Merging: This data is merged with the (https://www.4icu.org/us/) scraped information to get as much information about state location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529fb9d-1f1f-4e8c-b458-29aee7a6ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping from StudyAbroad.Shiksha.com to obtain States names along with University names\n",
    "\n",
    "# Etracting from this website https://studyabroad.shiksha.com/usa/universities-10\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "webdriver_location = r'C:\\Users\\Lenovo V15\\Downloads\\chromedriver_win32.exe'\n",
    "\n",
    "# Create a Service object with the webdriver_location\n",
    "service = Service(executable_path=webdriver_location)\n",
    "\n",
    "# Pass the Service object to the webdriver.Chrome constructor\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "scraped_pages = []\n",
    "base_url = \"https://studyabroad.shiksha.com/usa/universities\"\n",
    "\n",
    "for i in range(50):\n",
    "    # Update URL based on the current iteration\n",
    "    url = base_url if i == 0 else f'{base_url}-{i + 1}'\n",
    "    \n",
    "    #print(f\"Scraping page {i + 1}: {url}\")\n",
    "    \n",
    "    driver.get(url)\n",
    "    uni_html = driver.page_source\n",
    "    scraped_pages.append(BeautifulSoup(uni_html, 'html.parser'))\n",
    "\n",
    "# Close the driver after the loop\n",
    "driver.quit()\n",
    "\n",
    "# Double check the pages scraping to confirm that all have been scraped (50)\n",
    "print(f\"Number of pages scraped: {len(scraped_pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49524c-96ca-46bb-859f-c5c3deb06318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store final results in a dataframe after getting all the soups\n",
    "\n",
    "df_final = pd.DataFrame(columns=['School', 'City'])\n",
    "\n",
    "for scrape in scraped_pages:\n",
    "    school_divs = scrape.find_all('tr')\n",
    "\n",
    "    universities = []\n",
    "    locations = []\n",
    "\n",
    "    for div in school_divs:\n",
    "        # Find the university name\n",
    "        uni_name_tag = div.find('a', class_='font-15')\n",
    "    \n",
    "        # Find the city/state information\n",
    "        city_state_tag = div.find_all('td')\n",
    "    \n",
    "        if uni_name_tag and len(city_state_tag) >= 3:\n",
    "            universities.append(uni_name_tag.text.strip())\n",
    "            locations.append(city_state_tag[2].text.strip())\n",
    "\n",
    "    # Create a DataFrame for the current scrape\n",
    "    df = pd.DataFrame({'School': universities, 'City': locations})\n",
    "    \n",
    "    # Concatenate the current DataFrame with the final DataFrame\n",
    "    df_final = pd.concat([df_final, df], ignore_index=True)\n",
    "\n",
    "# Print the final DataFrame\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a2411-b9f2-4ad6-856a-ebfd84ed9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[['City', 'State']] = df_final['City'].str.split(',', expand=True)\n",
    "df_final['State'] = df_final['State'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138701a-c050-4e3a-b5e4-75eec15bee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a .csv file\n",
    "df_final.to_csv('USA_Uni_City_State_Location.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a9706-cdfe-4b3e-9aa9-cb32bce21fe1",
   "metadata": {},
   "source": [
    "**Combine the two dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419515e8-94dd-4a37-befc-d652e0d989d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dfs once more and merge\n",
    "df_USA_Uni_Rank = pd.read_csv(\"USA_Uni_Rank.csv\")\n",
    "df_USA_Uni_Location = pd.read_csv(\"USA_Uni_City_State_Location.csv\")\n",
    "df_UniRank_Location = pd.merge(df_USA_Uni_Rank, df_USA_Uni_Location, left_on='School', right_on='School', how='left')\n",
    "df_UniRank_Location.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7ccdc-11d2-4d22-91c2-c0f440ef71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UniRank_Location.to_csv(\"USA_Uni_Rank_City_State_Index.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
